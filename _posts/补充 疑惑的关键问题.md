# 一、概率论
## 1.正态分布
### 1.1 一维分布
```math
\mathcal{N}\left(x ; \mu, \sigma^{2}\right)=\sqrt{\frac{1}{2 \pi \sigma^{2}}} \exp \left(-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}\right)
```
- 精度表示 `$\beta=\frac{1}{\sigma^2}$`
```math
\mathcal{N}\left(x ; \mu, \beta^{-1}\right)=\sqrt{\frac{\beta}{2 \pi}} \exp \left(-\frac{1}{2} \beta(x-\mu)^{2}\right)
```

### 1.2 选择正态分布的原因
- 中心极限定理说明很多独立随机变量的和近似服从正态分布
- 在具有相同方差的所有可能的概率分布中，正态分布在实数上具有最大的不确定性。因此，我们可以认为正态分布是对模型加入的先验知识量最少的分布。

### 1.3 多维正态分布
```math
\mathcal{N}(x ; \mu, \Sigma)=\sqrt{\frac{1}{(2 \pi)^{n} \operatorname{det}(\Sigma)}} \exp \left(-\frac{1}{2}(x-\mu)^{\top} \Sigma^{-1}(x-\mu)\right)
```
- `$\Sigma$`表示正态分布的协方差矩阵
- 为避免对其求逆，采用精度矩阵进行描述
```math
\mathcal{N}\left(x ; \mu, \beta^{-1}\right)=\sqrt{\frac{\operatorname{det}(\beta)}{(2 \pi)^{n}}} \exp \left(-\frac{1}{2}(x-\mu)^{\top} \beta(x-\mu)\right)
```

## 2.先验和后验
- **先验** `$P(c)$`一词表明了在观测到x之前传递给模型关于c的信念。
- 作为对比，`$P(c|x)$`是**后验概率**（posterior probability），因为它是在观测到x之后进行计算的

## 3.常用函数
### 3.1 logistic sigmoid函数
```math
\sigma(x)=\frac{1}{1+\exp (-x)}
```
- sigmoid 函数在变量取绝对值非常大的正值或负值时会出现饱和（saturate）现象，意味着函数会变得很平，并且**对输入的微小改变会变得不敏感**

### 3.2 softplus函数
```math
\zeta(x)=\log (1+\exp (x))
```

### 3.3 相关性质
```math
\sigma(x)=\frac{\exp (x)}{\exp (x)+\exp (0)}\\
\frac{d}{d x} \sigma(x)=\sigma(x)(1-\sigma(x))\\

\begin{aligned}
1-\sigma(x) &=\sigma(-x) \\
\log \sigma(x) &=-\zeta(-x) \\
\frac{d}{d x} \zeta(x) &=\sigma(x)
\end{aligned}

```
### 3.4 贝叶斯规则
已知`$P(\mathrm{y}|\mathrm{x})$`和`$P(\mathrm{x})$`
- 求解`$P(\mathrm{y})$`
```math
P(\mathrm{y})=\sum_{\mathrm{x}} P(\mathrm{y} | \mathrm{x}) P(\mathrm{x})
```
```math
P(\mathrm{x} | \mathrm{y})=\frac{P(\mathrm{x}) P(\mathrm{y} | \mathrm{x})}{P(\mathrm{y})}
```

### 3.5 具有函数关系的连续性变量
假设我们有两个随机变量x 和y 满足y = g(x)，其中g 是可逆的、
连续可微的函数

- 标量情况
```math
p_{y}(y)=p_{x}\left(g^{-1}(y)\right)\left|\frac{\partial x}{\partial y}\right| \\

p_{x}(x)=p_{y}(g(x))\left|\frac{\partial g(x)}{\partial x}\right|
```

- 在高维空间中，微分运算扩展为Jacobian 矩阵的行列式——`$J_{i, j}=\frac{\partial x_{i}}{\partial y_{j}}$`
```math
p_{x}(x)=p_{y}(g(x))\left|\operatorname{det}\left(\frac{\partial g(x)}{\partial x}\right)\right|
```

# 二、信息论
对一个信号包含信息的多少进行量化

信息论的三个基本性质：
- 非常可能发生的事件信息量要比较少，确定发生的事情信息量为0
- 较不可能发生的事件具有更高的信息量
- 独立事件应具有增量的信息

## 2.1 自信息
```math
I(x)=-\log P(x)
```

## 2.2 香农熵
自信息只处理单个的输出,用香农熵(Shannon entropy)来对整个概率分布中的**不确定性总量**进行量化：
```math
H(\mathrm{x})=\mathbb{E}_{\mathrm{x} \sim P}[I(x)]=-\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)]=-\int p(x)\cdot \log p(x)dx
```
## 2.3 KL散度(Kullback-Leibler (KL) divergence)
对于同一个随机变量x有两个单独的概率分布P(x)和Q(x)，使用KL散度来衡量这**两个分布的差异**
```math
D_{\mathrm{KL}}(P \| Q)=\mathbb{E}_{\mathrm{x} \sim P}\left[\log \frac{P(x)}{Q(x)}\right]=\mathbb{E}_{\mathrm{x} \sim P}[\log P(x)-\log Q(x)]
```

## 2.4 交叉熵（cross entropy）
```math
H(P, Q)=-\mathbb{E}_{\mathbf{x} \sim P} \log Q(x)=-\int p(x)\log q(x)dx
```

# 三、结构化概率模型
- 把概率分布分解成许多因子的乘积形式，这种分解可以极大地减少用来描述一个分布的参数数量。
- 使用图描述这种分解，图模型(graphical model)
- 图的每一个节点对应一个随机变量，连接两个随机变量的边意味着概率分布可以表示成这两个随机变量之间的直接作用
## 3.1 有向图
![](https://cdn.jsdelivr.net/gh/Mronne/MarkDownImg/img/20200207114405.png)

## 3.2 无向图
- G 中任何满足两两之间有边连接的顶点的集合被称为团
- 每个团`$C^{(i)}$`都伴随一个因子`$\phi^{(i)}(C^{(i)})$`
![](https://cdn.jsdelivr.net/gh/Mronne/MarkDownImg/img/20200207120000.png)
